"""
ExTESæ•°æ®é›†å¤„ç†æ¨¡å—
"""
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datasets import Dataset, DatasetDict
from tqdm import tqdm

EXTES_STRATEGY_MAP = {
    "Reflective Statements": "åæ€æ€§é™ˆè¿°",
    "Clarification": "æ¾„æ¸…",
    "Emotional Validation": "æƒ…æ„ŸéªŒè¯",
    "Empathetic Statements": "å…±æƒ…é™ˆè¿°",
    "Affirmation": "è‚¯å®š",
    "Offer Hope": "ç»™äºˆå¸Œæœ›",
    "Avoid Judgment and Criticism": "é¿å…è¯„åˆ¤",
    "Suggest Options": "å»ºè®®é€‰é¡¹",
    "Collaborative Planning": "åä½œè§„åˆ’",
    "Provide Different Perspectives": "æä¾›ä¸åŒè§†è§’",
    "Reframe Negative Thoughts": "é‡æ„è´Ÿé¢æƒ³æ³•",
    "Share Information": "åˆ†äº«ä¿¡æ¯",
    "Normalize Experiences": "æ­£å¸¸åŒ–ä½“éªŒ",
    "Promote Self-Care Practices": "ä¿ƒè¿›è‡ªæˆ‘å…³æ€€",
    "Stress Management": "å‹åŠ›ç®¡ç†",
    "Others": "å…¶ä»–",
}

def load_extes_raw(data_path: str) -> List[Dict]:
    """åŠ è½½ExTES åŸå§‹JSONæ•°æ®

    Args:
        data_path (str): è·¯å¾„

    Returns:
        List[Dict]: å¯¹è¯åˆ—è¡¨
    """
    data_path = Path(data_path)

    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
    
    with open(data_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print("="*60)
    print(f"åŠ è½½äº†{len(data)} ä¸ªå¯¹è¯")
    print("="*60)
    return data


def _clean_text(value):
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, dict):
        for key in ("AI", "Response", "response", "text", "content"):
            v = value.get(key)
            if isinstance(v, str):
                return v.strip()
    return ""

def _clean_strategy(item, ai_value):
    for source in (ai_value if isinstance(ai_value, dict) else {}, item):
        if not isinstance(source, dict):
            continue
        for key in ("AI strategy", "AI Strategy", "strategy"):
            v = source.get(key)
            if isinstance(v, str):
                return v.strip()
    return ""

def parse_extes_dialog(dialog: Dict, index: int) -> Dict[str, Any]:
    """è§£æå•ä¸ªå¯¹è¯

    Args:
        dialog (Dict): åŸå§‹å¯¹è¯æ•°æ®
        index (int): å¯¹è¯çš„ç´¢å¼•å€¼

    Returns:
        Dict[str, Any]: æ ‡å‡†åŒ–çš„å¯¹è¯æ•°æ®
    """
    dialog_id = f"extes_{index:04d}"

    # æå–åœºæ™¯ä¿¡æ¯
    scene = dialog.get("scene", "")
    description = dialog.get("description", "")

    # è§£æå¯¹è¯å†…å®¹
    turns = []
    content = dialog.get("content", "")

    for item in content:
        if "User" in item:
            #ç”¨æˆ·ä¿¡æ¯
            text = _clean_text(item["User"])
            if text:
                turns.append({
                    "speaker": "seeker",
                    "text": text,
                    "strategy": ""
                })
        elif "AI" in item:
            #AIå›å¤
            ai_val = item["AI"]
            text = _clean_text(ai_val)
            strategy = _clean_strategy(item, ai_val)
            if text:
                turns.append({
                    "speaker": "supporter",
                    "text": text,
                    "strategy": strategy
                })
    
    return {
        "dialog_id": dialog_id,
        "scene": scene,
        "description": description,
        "turns": turns,
        "num_turns": len(turns)
    }



def build_rl_samples(
        dialogs: List[Dict[str, Any]],
        system_prompt: str,
        min_context_turns: int = 1,
        max_context_turns: int = 40,
        include_description: bool = True,
) -> List[Dict[str, Any]]:
    """æ„å»ºRL è®­ç»ƒæ ·æœ¬

    æ¯ä¸ªæ ·æœ¬åŒ…æ‹¬ï¼š
    - prompt: åˆ°å½“å‰è½®ä¸ºæ­¢çš„å¯¹è¯å†å²ï¼ˆç”¨äºæ¨¡å‹ç”Ÿæˆï¼‰
    - reference_response: çœŸå®çš„supporterå›å¤ï¼ˆç”¨äºè®¡ç®—å¥–åŠ±ï¼‰

    Args:
        dialogs (List[Dict[str, Any]]): è§£æåçš„å¯¹è¯åˆ—è¡¨
        system_prompt (str): ç³»ç»Ÿæç¤ºè¯
        min_context_turns (int, optional): æœ€å°ä¸Šä¸‹æ–‡è½®æ¬¡. Defaults to 1.
        max_context_turns (int, optional): æœ€å¤§ä¸Šä¸‹æ–‡è½®æ¬¡. Defaults to 40.
        include_description (bool, optional): _æ˜¯å¦åŒ…å«åœºæ™¯æè¿°. Defaults to True.

    Returns:
        List[Dict[str, Any]]: RLè®­ç»ƒæ ·æœ¬
    """
    all_samples = []

    for dialog in tqdm(dialogs, desc="æ„å»ºRLæ ·æœ¬"):
        turns = dialog["turns"]

        #æ„å»ºç³»ç»Ÿæç¤º
        if include_description and dialog["description"]:
            full_system_prompt = f"{system_prompt}\n\n ã€ç”¨æˆ·æƒ…æ™¯ã€‘{dialog['description']}"
        else:
            full_system_prompt = system_prompt
        
        # ä¸ºæ¯ä¸ª supporterå›å¤åˆ›å»ºRLæ ·æœ¬
        history = []

        for i, turn in enumerate(turns):
            if turn["speaker"] == "supporter":
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
                seeker_count = sum(1 for h in history if h["speaker"] == "seeker")

                if seeker_count >= min_context_turns:
                    # æ„å»º contextæ¶ˆæ¯ ï¼ˆé™åˆ¶æœ€å¤§è½®æ¬¡ï¼‰
                    context_history = history[-max_context_turns:]

                    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                    messages = [
                        {
                            "role": "system",
                            "content": full_system_prompt,
                        }
                    ]

                    for h in context_history:
                        role = "user" if h["speaker"] == "seeker" else "assistant"
                        messages.append({
                            "role": role,
                            "content": h["text"]
                        })

                    all_samples.append({
                        "dialog_id": dialog["dialog_id"],
                        "turn_index": i,
                        "messages": messages,
                        "reference_response": turn["text"],
                        "strategy": turn["strategy"],
                        "scene": dialog["scene"],
                        "context_turns": len(context_history)
                    })
            history.append(turn)
        
    print("="*60)
    print(f"å…±ç”Ÿæˆ {len(all_samples)} ä¸ª RLè®­ç»ƒæ ·æœ¬")
    print("="*60)
    return all_samples


# =====å¤‡ç”¨æ–¹æ¡ˆ=====
def build_rl_samples_from_esconv(
        esconv_path: str,
        system_prompt: str,
        min_context_turns: int = 1
) -> List[Dict[str, Any]]:
    """ä»ESConvå¤„ç†åçš„æ•°æ®æ„å»ºRLæ ·æœ¬(æ›¿ä»£æ–¹æ¡ˆ)

    Args:
        esconv_path (str): ESConv processedæ•°æ®
        system_prompt (str): ç³»ç»Ÿæç¤ºè¯
        min_context_turns (int, optional): æœ€å°ä¸Šä¸‹æ–‡è½®æ¬¡. Defaults to 1.

    Returns:
        List[Dict[str, Any]]: RL æ ·æœ¬
    """
    from datasets import load_from_disk

    esconv_dataset = load_from_disk(esconv_path)
    all_samples = []

    for split in ["train", "validation"]:
        for sample in tqdm(esconv_dataset[split], desc=f"ä» ESConv {split} æ„å»ºRLæ ·æœ¬"):
            messages = json.loads(sample["messages"])

            # è¿‡æ»¤ï¼šè‡³å°‘éœ€è¦ system + min_context_turns ä¸ªç”¨æˆ·æ¶ˆæ¯
            user_count = sum(1 for m in messages if m["role"] == "user")

            if user_count >= min_context_turns:
                all_samples.append({
                    "dialog_id": sample["dialog_id"],
                    "turn_index": sample["turn_index"],
                    "messages": messages,
                    "reference_response": sample["target_response"],
                    "strategy": sample["strategy"],
                    "scene": sample.get("problem_type", ""),
                    "context_turns": len(messages) -1
                })
    
    print("="*60)
    print(f" ä» ESConv ç”Ÿæˆ {len(all_samples)} ä¸ª RL è®­ç»ƒæ ·æœ¬")
    print("="*60)
    return all_samples



def split_rl_dataset(
        samples: List[Dict[str, Any]],
        train_ratio: float = 0.9,
        val_ratio: float = 0.1,
        seed: int = 42
) -> Tuple[List[Dict], List[Dict]]:
    """
    åˆ’åˆ†RLæ•°æ®é›†
    """
    import random
    random.seed(seed)

    # æŒ‰å¯¹è¯IDåˆ’åˆ†
    dialog_samples = {}
    for sample in samples:
        dialog_id = sample["dialog_id"]
        if dialog_id not in dialog_samples:
            dialog_samples[dialog_id] = []
        dialog_samples[dialog_id].append(sample)
    
    dialog_ids = list(dialog_samples.keys())
    random.shuffle(dialog_ids)

    n_train = int(len(dialog_ids) * train_ratio)
    train_ids = dialog_ids[:n_train]
    val_ids = dialog_ids[n_train:]

    train_samples = [s for did in train_ids for s in dialog_samples[did]]
    val_samples = [s for did in val_ids for s in dialog_samples[did]]

    print("="*60)
    print(f" RLæ•°æ®é›†åˆ’åˆ†ï¼š")
    print(f" è®­ç»ƒé›†ï¼š{len(train_samples)}æ ·æœ¬ ({len(train_ids)} å¯¹è¯)")
    print(f" éªŒè¯é›†ï¼š{len(val_samples)}æ ·æœ¬ ({len(val_ids)} å¯¹è¯)")

    return train_samples, val_samples



def create_hf_dataset(
        samples: List[Dict[str, Any]]
) -> Dataset:
    """è½¬æ¢æˆhuggingface datasetæ ¼å¼"""
    processed_samples = []
    for sample in samples:
        processed_samples.append(
            {
                "dialog_id": sample["dialog_id"],
                "turn_index": sample["turn_index"],
                "messages": json.dumps(sample["messages"], ensure_ascii=False),
                "reference_response": sample["reference_response"],
                "strategy": sample["strategy"],
                "scene": sample.get("scene", ""),
                "context_turns": sample["context_turns"]
            }
        )
    return Dataset.from_list(processed_samples)



def load_extes(
        data_path: str = "data/extes/raw/ExTES.json",
        system_prompt: Optional[str] = None,
        include_description: bool = True,
        use_esconv_fallback: bool = True,
        esconv_processed: bool = "data/esconv/processed",
        save_processed: bool = True,
        processed_dir: str = "data/extes/processed"
) -> DatasetDict:
    """åŠ è½½å¹¶å¤„ç† ExTES æ•°æ®é›†

    Args:
        data_path (str, optional): åŸå§‹æ•°æ®é›†. Defaults to "data/extes/raw/ExTES.json".
        system_prompt (Optional[str], optional): ç³»ç»Ÿæç¤ºè¯. Defaults to None.
        include_description (bool, optional): æ˜¯å¦åŒ…å«åœºæ™¯æè¿°. Defaults to True.
        use_esconv_fallback (bool, optional): å½“ExTESä¸å¯ç”¨æ—¶æ˜¯å¦ä½¿ç”¨ESConv. Defaults to True.
        esconv_processed (bool, optional): å·²ç»é¢„å¤„ç†ESConvæ•°æ®é›†ä½ç½®. Defaults to "data/esconv/processed".
        save_processed (bool, optional): æ˜¯å¦ä¿å­˜å¤„ç†åçš„æ•°æ®é›†. Defaults to True.
        processed_dir (str, optional): ä¿å­˜å¤„ç†åçš„æ•°æ®é›†çš„ä½ç½®. Defaults to "data/extes/processed".

    Returns:
        DatasetDict: huggingfaceæ•°æ®é›†
    """
    if system_prompt is None:
        system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿæ”¯æŒåŠ©æ‰‹ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
    1. è®¤çœŸå€¾å¬ç”¨æˆ·çš„å›°æ‰°å’Œæƒ…ç»ª
    2. è¡¨è¾¾çœŸè¯šçš„ç†è§£å’Œå…±æƒ…
    3. ä½¿ç”¨æ°å½“çš„æƒ…æ„Ÿæ”¯æŒç­–ç•¥ï¼ˆå¦‚æé—®ã€é‡Šä¹‰ã€è‚¯å®šã€å»ºè®®ç­‰ï¼‰
    4. å¸®åŠ©ç”¨æˆ·æ¢ç´¢é—®é¢˜ã€èˆ’ç¼“æƒ…ç»ªã€æ‰¾åˆ°è§£å†³æ–¹å‘

    è¯·ç”¨æ¸©æš–ã€ä¸“ä¸šã€çœŸè¯šçš„æ–¹å¼ä¸ç”¨æˆ·äº¤æµã€‚
    """
    
    print("="*60)
    print("ExTESæ•°æ®é›†å¤„ç†")
    print("="*60)

    all_samples = []
    data_source = "unknown"

    extes_path = Path(data_path)
    if extes_path.exists():
        print(f"\n åŠ è½½ ExTES æ•°æ®é›† ï¼š{extes_path}")

        try:
            raw_data = load_extes_raw(extes_path)

            # è§£æå¯¹è¯
            print("\n è§£æå¯¹è¯...")
            parsed_dialogs = []
            for idx, d in enumerate(tqdm(raw_data, desc="è§£æå¯¹è¯")):
                parsed = parse_extes_dialog(d, idx)
                parsed_dialogs.append(parsed)
            
            # è¿‡æ»¤æœ‰æ•ˆå¯¹è¯
            valid_dialogs = [d for d in parsed_dialogs if d["num_turns"] >= 2]
            print(f"    æœ‰æ•ˆå¯¹è¯ï¼š{len(valid_dialogs)} / {len(parsed_dialogs)}")

            if valid_dialogs:
                # æ„å»º RL æ ·æœ¬
                print("\n æ„å»ºRLæ ·æœ¬...")

                all_samples = build_rl_samples(
                    valid_dialogs,
                    system_prompt,
                    include_description=include_description
                )
                data_source = "ExTES"

        except Exception as e:
            print(f"\n ExTESæ•°æ®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{e}")
    
    else:
        print(f"ExTESæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{extes_path}")
    
    # é’ˆå¯¹ExTESçš„å¼‚å¸¸å¤„ç†
    # å¦‚æœ ExTES æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨ ESConv è¡¥å……
    if len(all_samples) < 100 and use_esconv_fallback:
        print(f"\n ä½¿ç”¨ ESConv ä½œä¸º RL æ•°æ®æº...")
        try:
            esconv_samples = build_rl_samples_from_esconv(esconv_processed, system_prompt)
            if not all_samples:
                all_samples = esconv_samples
                data_source = "ESConv"
            else:
                all_samples.extend(esconv_samples)
                data_source = "ExTES+ESConv"
        except Exception as e:
            print(f" ESConv åŠ è½½å¤±è´¥: {e}")
    
    if not all_samples:
        raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„RLè®­ç»ƒæ•°æ®é›†ï¼è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
    
    print(f"\n æ•°æ®æ¥æºï¼š{data_source}")

    # åˆ’åˆ†æ•°æ®é›†
    print("\n åˆ’åˆ†æ•°æ®é›†...")
    train_samples, val_samples = split_rl_dataset(all_samples)

    # åˆ›å»º DatasetDict
    dataset_dict = DatasetDict({
        "train": create_hf_dataset(train_samples),
        "validation": create_hf_dataset(val_samples)
    })

    # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†
    if save_processed:
        processed_path = Path(processed_dir)
        processed_path.mkdir(parents=True, exist_ok=True)
        dataset_dict.save_to_disk(str(processed_path))
        print(f"\n æ•°æ®å·²ä¿å­˜åˆ°ï¼š{processed_path}")
    
    print("\n" + "="*60)
    print(" RL æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    print("=" * 60)

    return dataset_dict



def load_processed_extes(
        processed_dir: str = "data/extes/processed"
) -> DatasetDict:
    """åŠ è½½å·²å¤„ç†çš„ExTES æ•°æ®é›†"""
    from datasets import load_from_disk

    processed_path = Path(processed_dir)
    if not processed_path.exists():
        raise FileNotFoundError(f"å¤„ç†åçš„æ•°æ®ä¸å­˜åœ¨ï¼š{processed_dir}")
    
    dataset = load_from_disk(str(processed_path))
    print(f"\n  å·²åŠ è½½RLæ•°æ®é›†")
    print(f"    è®­ç»ƒé›†ï¼š{len(dataset['train'])} æ ·æœ¬")
    print(f"    éªŒè¯é›†ï¼š{len(dataset['validation'])}")

    return dataset



def get_sample(
        dataset: DatasetDict,
        split: str = "train",
        index: int = 0
) -> Dict:
    """è·å–ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹"""
    sample = dict(dataset[split][index])
    sample["messages"] = json.loads(sample["messages"])
    return sample



# ==========ç»Ÿè®¡åˆ†æå‡½æ•°==========

def analyze_extes_dataset(
        dataset: DatasetDict
) -> Dict[str, Any]:
    """åˆ†æç»Ÿè®¡æƒ…å†µ"""
    stats = {
        "splits": {},
        "strategies": {},
        "scenes": {},
        "context_turns": [],
        "response_lengths": []
    }

    for split_name in dataset.keys():
        split_data = dataset[split_name]
        stats["splits"][split_name] = len(split_data)

        for sample in split_data:
            # ç»Ÿè®¡ç­–ç•¥
            strategy = sample["strategy"]
            if strategy:
                stats["strategies"][strategy] = stats["strategies"].get(strategy, 0) + 1
            
            # ç»Ÿè®¡åœºæ™¯
            scene = sample.get("scene", "")
            if scene:
                stats["scenes"][scene] = stats["scenes"].get(scene, 0) + 1
            
            # ç»Ÿè®¡ä¸Šä¸‹æ–‡
            stats["context_turns"].append(sample["context_turns"])

            stats["response_lengths"].append(len(sample["reference_response"]))
    
    if stats["context_turns"]:
        stats["avg_context_turns"] = sum(stats["context_turns"]) / len(stats["context_turns"])

    if stats["response_lengths"]:
        stats["avg_response_length"] = sum(stats["response_lengths"]) / len(stats["response_lengths"])

    return stats



def print_extes_stats(stats: Dict[str, Any]):
    """æ‰“å°ç»Ÿè®¡æ•°æ®"""
    print("\n" + "=" * 60)
    print(" ExTES/RL æ•°æ®é›†ç»Ÿè®¡")
    print("=" * 60)
    
    print("\nã€æ•°æ®åˆ’åˆ†ã€‘")
    for split, count in stats["splits"].items():
        print(f"  {split}: {count} æ ·æœ¬")
    
    print(f"\nã€å¹³å‡ç»Ÿè®¡ã€‘")
    print(f"  å¹³å‡ä¸Šä¸‹æ–‡è½®æ¬¡: {stats.get('avg_context_turns', 0):.1f}")
    print(f"  å¹³å‡å›å¤é•¿åº¦: {stats.get('avg_response_length', 0):.1f} å­—ç¬¦")
    
    if stats["strategies"]:
        print("\nã€ç­–ç•¥åˆ†å¸ƒ (Top 10)ã€‘")
        sorted_strategies = sorted(stats["strategies"].items(), key=lambda x: x[1], reverse=True)
        total = sum(stats["strategies"].values())
        for strategy, count in sorted_strategies[:10]:
            pct = 100 * count / total
            strategy_cn = EXTES_STRATEGY_MAP.get(strategy, strategy)
            print(f"  {strategy_cn}: {count} ({pct:.1f}%)")
    
    if stats["scenes"]:
        print("\nã€åœºæ™¯åˆ†å¸ƒ (Top 10)ã€‘")
        sorted_scenes = sorted(stats["scenes"].items(), key=lambda x: x[1], reverse=True)
        for scene, count in sorted_scenes[:10]:
            print(f"  {scene}: {count}")



if __name__ == "__main__":
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    
    #å¤„ç†RLæ•°æ®é›†
    dataset = load_extes(
        data_path="data/extes/raw/ExTES.json",
        use_esconv_fallback=False,
        save_processed=True
    )

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = analyze_extes_dataset(dataset)
    print_extes_stats(stats)
    
    # æ‰“å°ç¤ºä¾‹
    print("\n" + "=" * 60)
    print("ğŸ“ RL æ ·æœ¬ç¤ºä¾‹")
    print("=" * 60)
    
    sample = get_sample(dataset, "train", 0)
    print(f"\nå¯¹è¯ ID: {sample['dialog_id']}")
    print(f"åœºæ™¯: {sample['scene']}")
    print(f"ä¸Šä¸‹æ–‡è½®æ¬¡: {sample['context_turns']}")
    print(f"ç­–ç•¥: {sample['strategy']}")
    print(f"\nå†å²æ¶ˆæ¯:")
    for msg in sample['messages']:
        role = {"system": "ç³»ç»Ÿ", "user": "ç”¨æˆ·", "assistant": "åŠ©æ‰‹"}[msg["role"]]
        content = msg['content'][:80] + "..." if len(msg['content']) > 80 else msg['content']
        print(f"  [{role}] {content}")
    print(f"\nå‚è€ƒå›å¤: {sample['reference_response']}")


