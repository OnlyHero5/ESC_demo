"""
ESC-RANK 评分模块

使用 ESC-RANK 对多轮对话进行 7 维度评分
"""

import json
import torch
from pathlib import Path
from typing import Dict, List, Any, Optional
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftMixedModel

DIMENSION_PROMPTS_EN = {
    "fluency": (
        "You are an expert annotator for dialogue systems. Please evaluate the **Fluency** of the AI assistant's responses in the conversation context.\n\n"
        "**Definition**:\n"
        "Fluency is evaluated based on two aspects:\n"
        "1. Intra-sentence fluency: Are the sentences grammatically correct and easy to read? (No truncation or broken text).\n"
        "2. Inter-sentence coherence: Is the logical flow natural? Do the responses directly address the user's previous input?\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Unintelligible. Sentences are broken or completely unrelated to the context.\n"
        "- 1: Poor. Frequent grammatical errors or severe logic jumps that confuse the user.\n"
        "- 2: Fair. Understandable, but contains noticeable phrasing awkwardness or slight logic inconsistencies.\n"
        "- 3: Good. Fluent and smooth, with no obvious errors or coherence issues.\n"
        "- 4: Excellent. Perfectly fluent, logical, and polished, exhibiting native-level proficiency.\n\n"
        "Please strictly follow this format: First, provide a brief **Explanation** analyzing the fluency. Then, output the **Score**."
    ),
    "diversity": (
        "You are an expert annotator. Please evaluate the **Diversity** of the AI assistant's responses.\n\n"
        "**Definition**:\n"
        "Diversity assesses whether the AI avoids repetition and employs a rich vocabulary, varied sentence structures, and distinct response patterns.\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Extremely repetitive. Uses the exact same phrases or templates repeatedly.\n"
        "- 1: Low. Very limited vocabulary; responses feel generic (e.g., 'I understand' multiple times).\n"
        "- 2: Moderate. Some variation, but relies on safe/common patterns.\n"
        "- 3: Good. Uses varied vocabulary and sentence structures appropriately.\n"
        "- 4: Excellent. Highly creative, novel, and engaging expressions; no repetitive patterns detected.\n\n"
        "Format: **Explanation** followed by **Score**."
    ),
    "empathic": (
        "You are an expert annotator specialized in psychology. Please evaluate the **Empathy** of the AI assistant.\n\n"
        "**Definition**:\n"
        "Empathy measures how well the AI perceives the user's emotional state, validates their feelings, and responds with warmth and understanding. It involves active listening and emotional resonance.\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Cold/Apathetic. Ignores feelings or is rude/dismissive.\n"
        "- 1: Mechanical. Uses robotic scripts to acknowledge emotion (e.g., 'I am sorry to hear that') without depth.\n"
        "- 2: Surface-level. Acknowledges emotions correctly but lacks deeper understanding or warmth.\n"
        "- 3: Empathetic. Clearly understands feelings and validates them effectively.\n"
        "- 4: Deeply Empathetic. Demonstrates profound understanding, makes the user feel genuinely heard, cared for, and supported.\n\n"
        "Format: **Explanation** followed by **Score**."
    ),
    "suggestion": (
        "You are an expert annotator. Please evaluate the **Suggestion Effectiveness** of the AI assistant.\n\n"
        "**Definition**:\n"
        "This dimension evaluates whether the AI provides advice that is practical, feasible, and relevant to the user's specific problem. Note: In ESC, suggestions should only be given after sufficient empathy is established.\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Useless/Harmful. Suggestions are irrelevant, dangerous, or completely missing when needed.\n"
        "- 1: Generic. Cliché advice (e.g., 'Just relax') that provides no real value.\n"
        "- 2: Average. Advice is relevant but somewhat vague or not tailored to the user's specific context.\n"
        "- 3: Helpful. Concrete and actionable advice that addresses the core issue.\n"
        "- 4: Outstanding. Highly personalized, actionable, and insightful advice that perfectly fits the user's situation.\n\n"
        "Format: **Explanation** followed by **Score**."
    ),
    "human": (
        "You are an expert annotator. Please evaluate the **Human-likeness** of the AI assistant.\n\n"
        "**Definition**:\n"
        "Human-likeness assesses whether the AI communicates naturally, like a real person, avoiding 'AI-ese' (excessive formality, preaching, or robotic lists).\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Machine-like. Clearly generated by a template or bot.\n"
        "- 1: Rigid. Mostly stiff language with very few natural elements.\n"
        "- 2: Mixed. A blend of natural phrasing and robotic artifacts.\n"
        "- 3: Natural. Flows well, conversational tone, hard to distinguish from a casual human chatter.\n"
        "- 4: Indistinguishable. Perfect nuance, tone, and adaptability; passes the Turing test for this conversation.\n\n"
        "Format: **Explanation** followed by **Score**."
    ),
    "tech": (
        "You are an expert annotator in Emotional Support Conversation. Please evaluate the **Support Skills (Emotional Technique)** of the AI assistant.\n\n"
        "**Definition**:\n"
        "This measures the AI's ability to apply psychological support techniques (e.g., Exploration, Comforting, Action) appropriately. It assesses whether the AI identifies the problem correctly and guides the conversation professionally.\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: No skills. Fails to identify the problem or use any support strategy.\n"
        "- 1: Poor application. Attempts to use skills but misinterprets the user's intent or timing.\n"
        "- 2: Basic. Uses standard techniques (e.g., questioning) but lacks strategic progression.\n"
        "- 3: Proficient. Correctly identifies the problem and uses appropriate strategies to guide the user.\n"
        "- 4: Expert. Masterfully navigates the conversation using advanced techniques to foster emotional improvement.\n\n"
        "Format: **Explanation** followed by **Score**."
    ),
    "overall": (
        "You are an expert annotator. Please provide an **Overall Quality** score for the AI assistant's performance.\n\n"
        "**Definition**:\n"
        "Consider all dimensions: Fluency, Empathy, Helpfulness (Suggestions), and Naturalness. Did the AI successfully provide emotional support?\n\n"
        "**Scoring Scale (0-4)**:\n"
        "- 0: Failure. The conversation failed to provide any support or was incoherent.\n"
        "- 1: Poor. Significant flaws in empathy or logic; bad user experience.\n"
        "- 2: Fair. Passable interaction, but lacks depth or personalization.\n"
        "- 3: Good. A satisfying interaction that met the user's basic emotional needs.\n"
        "- 4: Excellent. A high-quality, professional-grade emotional support session.\n\n"
        "Format: **Explanation** followed by **Score**."
    )
}

DIMENSION_PROMPTS_CN = {
    "fluency": (
        "你是一名对话系统评估专家。请评估 AI 助手在对话中的**流畅性 (Fluency)**。\n\n"
        "**定义**：\n"
        "流畅性主要从两个方面评估：\n"
        "1. 句内流畅性：句子是否语法正确且通顺？（无截断或语义破碎）。\n"
        "2. 句间连贯性：逻辑流是否自然？回复是否直接关联用户的上下文？\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：无法理解。句子破碎或与上下文完全无关。\n"
        "- 1：差。存在频繁的语法错误或严重的逻辑跳跃，导致用户困惑。\n"
        "- 2：一般。可以理解，但存在明显的措辞生硬或轻微的逻辑不连贯。\n"
        "- 3：好。流畅通顺，没有明显的错误或连贯性问题。\n"
        "- 4：极佳。完美流畅，逻辑严密，表达润色得当，达到母语水平。\n\n"
        "格式要求：请先提供简短的**解释**分析流畅性，然后给出**分数**。"
    ),
    "diversity": (
        "你是一名评估专家。请评估 AI 助手回复的**多样性 (Diversity)**。\n\n"
        "**定义**：\n"
        "多样性评估 AI 是否避免了重复，并使用了丰富的词汇、多变的句式结构和独特的回复模式。\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：极度重复。反复使用完全相同的短语或模板。\n"
        "- 1：低。词汇量非常有限；回复感觉很通用（例如多次使用“我理解”）。\n"
        "- 2：中等。有一些变化，但依赖于安全/常见的模式。\n"
        "- 3：好。恰当地使用了多样的词汇和句式。\n"
        "- 4：极佳。极具创造性、新颖且引人入胜的表达；未检测到重复模式。\n\n"
        "格式要求：先**解释**，后**评分**。"
    ),
    "empathic": (
        "你是一名心理学领域的评估专家。请评估 AI 助手的**共情能力 (Empathy)**。\n\n"
        "**定义**：\n"
        "共情能力衡量 AI 感知用户情绪状态、验证其感受以及做出温暖且理解性回应的程度。它涉及积极倾听和情感共鸣。\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：冷漠/无情。忽视感受，或表现得粗鲁/敷衍。\n"
        "- 1：机械。使用机器人式的脚本来确认情绪（例如“听到这个我很抱歉”），缺乏深度。\n"
        "- 2：表面层面。正确确认了情绪，但缺乏深层理解或温暖感。\n"
        "- 3：有共情。清晰地理解感受并有效地进行了验证。\n"
        "- 4：深度共情。表现出深刻的理解，让用户感到真正被倾听、被关心和被支持。\n\n"
        "格式要求：先**解释**，后**评分**。"
    ),
    "suggestion": (
        "你是一名评估专家。请评估 AI 助手的**建议有效性 (Suggestion Effectiveness)**。\n\n"
        "**定义**：\n"
        "该维度评估 AI 提供的建议是否实用、可行，并与用户的具体问题相关。注意：在情绪支持对话中，建议应在建立足够的共情之后提出。\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：无用/有害。建议不相关、危险，或在需要时完全缺失。\n"
        "- 1：通用。陈词滥调的建议（如“放松点”），没有实际价值。\n"
        "- 2：平均。建议相关，但有些模糊或未针对用户的具体情况进行调整。\n"
        "- 3：有帮助。针对核心问题提出了具体且可操作的建议。\n"
        "- 4：杰出。高度个性化、可操作且富有洞察力的建议，完美契合用户的情境。\n\n"
        "格式要求：先**解释**，后**评分**。"
    ),
    "human": (
        "你是一名评估专家。请评估 AI 助手的**拟人度 (Human-likeness)**。\n\n"
        "**定义**：\n"
        "拟人度评估 AI 的交流是否自然，像真人一样，避免“AI 腔”（过度的形式主义、说教或机器人式的列表）。\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：机器化。明显由模板或机器人生成。\n"
        "- 1：僵硬。大部分语言生硬，极少有自然元素。\n"
        "- 2：混合。自然表达与机器人痕迹的混合。\n"
        "- 3：自然。行文流畅，具有对话语调，难以与随意的真人聊天区分。\n"
        "- 4：无法区分。完美的细微差别、语调和适应性；在本次对话中通过了图灵测试。\n\n"
        "格式要求：先**解释**，后**评分**。"
    ),
    "tech": (
        "你是一名情绪支持对话专家。请评估 AI 助手的**支持技巧 (Support Skills / Emotional Technique)**。\n\n"
        "**定义**：\n"
        "该维度衡量 AI 恰当应用心理支持技巧（如探索、安抚、行动建议）的能力。它评估 AI 是否正确识别了问题并专业地引导对话。\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：无技巧。未能识别问题或未使用任何支持策略。\n"
        "- 1：应用不当。尝试使用技巧，但误解了用户的意图或时机。\n"
        "- 2：基础。使用标准技巧（如提问），但缺乏策略性的递进。\n"
        "- 3：熟练。正确识别问题并使用适当的策略引导用户。\n"
        "- 4：专家级。巧妙地引导对话，使用高级技巧促进情绪改善。\n\n"
        "格式要求：先**解释**，后**评分**。"
    ),
    "overall": (
        "你是一名评估专家。请对 AI 助手的**整体质量 (Overall Quality)** 进行评分。\n\n"
        "**定义**：\n"
        "综合考虑所有维度：流畅性、共情能力、帮助性（建议）和自然度。AI 是否成功提供了情绪支持？\n\n"
        "**评分标准 (0-4)**：\n"
        "- 0：失败。对话未能提供任何支持，或逻辑混乱。\n"
        "- 1：差。在共情或逻辑上存在重大缺陷；用户体验糟糕。\n"
        "- 2：一般。尚可的互动，但缺乏深度或个性化。\n"
        "- 3：好。令人满意的互动，满足了用户的基本情绪需求。\n"
        "- 4：优秀。高质量、专业级的情绪支持会话。\n\n"
        "格式要求：先**解释**，后**评分**。"
    )
}

class ESCRank:
    """
    ESC-RANK 评分模型

    基于 InternLM2 + 7个维度的 LoRA adapters
    """
    def __init__(
            self,
            base_model_path: str,
            adapter_path: str,
            torch_dtype: str = "bfloat16",
            device_map: str = "auto",
            language: str = "en"
            ):
        """初始化 ESC-RANK 模型

        Args:
            base_model_path (str): InternLM2 基座模型
            adapter_path (str): ESC-RANK LoRA adapters 路径
            torch_dtype (str, optional): 数据类型. Defaults to "bfloat16".
            device_map (str, optional): 设备映射. Defaults to "auto".
            language (str, optional): 语言. Defaults to "en".
        """
        self.language = language
        self.adapter_path = Path(adapter_path)

        # 数据类型
        dtype_map = {
            "bfloat16": torch.bfloat16,
            "float16": torch.float16,
            "float32": torch.float32
        }
        torch_dtype_val = dtype_map.get(torch_dtype, torch.bfloat16)

        print(f"加载 ESC-RANK 基座模型：{base_model_path}")

        # 加载 Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code = True
        )

        # 加载 基座模型
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=torch_dtype_val,
            device_map=device_map
        ).eval()
    
        # 确定 adapter 后缀
        suffix = "_en" if language == "en" else ""

        # 加载第一个 adapter 初始化 PeftMixedModel
        first_adapter = f"fluency{suffix}"
        first_adapter_path = self.adapter_path / first_adapter

        print("加载lora adapters...")
        self.peft_model = PeftMixedModel.from_pretrained(
            self.model,
            str(first_adapter_path)
        )

        # 加载其他 adapters
        dimensions = ["fluency", "diversity", "empathic", "suggestion", "human", "tech", "overall"]
        for dim in dimensions:
            adapter_name = f"{dim}{suffix}"
            adapter_full_path = self.adapter_path / adapter_name
            if adapter_full_path.exists():
                self.peft_model.load_adapter(str(adapter_full_path), adapter_name=dim)
                print(f"    已经加载：{adapter_name}")

        self.dimensions = dimensions
        self.prompts = DIMENSION_PROMPTS_EN if language == "en" else DIMENSION_PROMPTS_CN

        print("ESC-RANK 模型加载完成")
    
    def score_dialogue(
            self,
            dialogue: List[str]) -> Dict[str, int]:
        """对单个对话进行7维度评分

        Args:
            dialogue (List[str]): 对话列表 ["ESC-Role: ...", "AI assistant: ...", ...]
            
        Returns:
            Dict[str, int]: 7维度评分
        """
        dialogue_text = "\n\n".join(dialogue)
        dialogue_text = dialogue_text.replace("AI assistant", "**AI助手**").replace("ESC-Role", "**用户**")

        scores = {}
        valid_labels = ["0", "1", "2", "3", "4"]

        for dim in self.dimensions:
            # 切换到对应的 adapter
            self.peft_model.set_adapter(dim)

            # 构建 prompt
            rule_prompt = self.prompts[dim]
            
            full_prompt = (
                f"{rule_prompt}\n\n"               # 1. 评分规则
                f"=== Conversation Start ===\n"     # 2. 对话开始标记
                f"{dialogue_text.strip()}\n"        # 3. 对话内容
                f"=== Conversation End ===\n\n"     # 4. 对话结束标记
                f"Evaluation Task:\n"
                f"Please provide your **Explanation** first, analyze the {dim} specifically, "
                f"and ends with 'Score: X' (where X is 0-4)." # 5. 强制指定结尾格式
            )
            # 构建消息
            messages = [{
                "role": "user",
                "content": full_prompt
            }]

            # 生成评分
            response = self._generate(messages)

            # 解析评分
            score = self._extract_score_from_text(response)
            # 异常则打印出来调试
            if score == -1:
                print(f"[Warning] Failed to parse score for {dim}. Response: {response[:50]}...")
            
            scores[dim] = score
        
        return scores

    def _extract_score_from_text(self, text: str) -> int:
        import re
        # 方法 A: 严格匹配 "Score: X" (最推荐，配合 Prompt 里的指令)
        match = re.search(r"Score:\s*(\d)", text, re.IGNORECASE)
        if match:
            s = int(match.group(1))
            if 0 <= s <= 4:
                return s

        # 方法 B: 如果模型没听话，没写 "Score:"，则寻找全得文本中最后一个单独的数字
        # 匹配所有 0-4 的数字
        candidates = re.findall(r"\b([0-4])\b", text)
        if candidates:
            # 取最后一个数字，通常 CoT 都是先分析后给分
            return int(candidates[-1])
            
        return -1 # 解析失败 

    def _generate(
            self,
            messages: List[Dict[str, str]]
    ) -> str:
        """生成评分回复"""
        query = messages[-1]["content"]

        # 构建输入 - 使用 apply_chat_template 而不是 .chat() 方法
        # 这样可以避免 KV Cache 不一致的问题
        chat_messages = [{"role": "user", "content": query}]
        
        text = self.tokenizer.apply_chat_template(
            chat_messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            [text],
            return_tensors="pt"
        ).to(self.peft_model.device)
        
        # 使用 generate 方法，不使用 cache
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False,
                temperature=None,  # greedy decoding
                top_p=None,
                use_cache=False,  # 禁用 KV Cache 避免位置编码错误
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # 解码生成的文本
        generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
       
        return response.strip()
    
    def score_all_dialogues(
            self,
            dialogues: Dict[str, List[str]],
            save_path: Optional[str] = None
    ) -> Dict[str, Dict[str, int]]:
        """对所有对话进行评分

        Args:
            dialogues (Dict[str, List[str]]): 对话词典 {index: dialogue_list}
            save_path (Optional[str], optional): 结果保存路径. Defaults to None.

        Returns:
            Dict[str, Dict[str, int]]: 评分结果 {index: {dim:score}}
        """
        results = {}

        for idx, dialogue in tqdm(dialogues.items(), desc="评分中"):
            scores = self.score_dialogue(dialogue)
            results[idx] = scores
        
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            print(f"评分结果已保存到：{save_path}")
        
        return results
    
    def unload(self):
        """卸载模型"""
        self.peft_model.to("cpu")
        del self.peft_model
        torch.cuda.empty_cache()
        print("ESC-RANK 模型已经卸载")

def compute_average_scores(scores: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    """计算各维度平均分

    Args:
        scores (Dict[str, Dict[str, int]]): 评分结果 {index: {dim: score}}

    Returns:
        Dict[str, float]: 平均分 {dim: avg_score}
    """

    dimensions = ["fluency", "diversity", "empathic", "suggestion", "human", "tech", "overall"]

    averages = {}
    for dim in dimensions:
        valid_scores = [s[dim] for s in scores.values() if s.get(dim, -1) >= 0]
        if valid_scores:
            averages[dim] = sum(valid_scores) / len(valid_scores)
        else:
            averages[dim] = 0.0

    return averages

def print_esc_rank_results(
        scores: Dict[str, float],
        model_name: str = "Model"
):
    """打印 ESC-RANK 评分结果"""
    print("\n" + "="*50)
    print(f"ESC-RANK 评分结果：{model_name}")
    print("="*60)

    dim_names = {
         "fluency": "流畅度",
        "diversity": "多样性",
        "empathic": "共情能力",
        "suggestion": "建议有效性",
        "human": "拟人度",
        "tech": "情感知识",
        "overall": "整体偏好"
    }

    for dim, score in scores.items():
        cn_name = dim_names.get(dim, dim)
        print(f"    {cn_name} ({dim}): {score:.2f} / 4.00")
    
    print("=" * 60)
    